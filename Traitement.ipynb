{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marya204/Recommendation-System-for-job-offers/blob/Processing/Traitement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **downloading dependencies**"
      ],
      "metadata": {
        "id": "wP6G3KAE9JIF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLb44FwBL8ML"
      },
      "outputs": [],
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67PGuWKfWnlr"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt-get remove -y chromium-browser\n",
        "!apt-get autoremove -y\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -y --fix-broken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install webdriver-manager"
      ],
      "metadata": {
        "id": "wr4TegpwgiYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Emploima Scraper**"
      ],
      "metadata": {
        "id": "egDQLQAGvsew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Import webdriver_manager to handle Chromedriver installation\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "def init_driver():\n",
        "    \"\"\"Initialise le driver Chrome\"\"\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
        "    chrome_options.binary_location = '/usr/bin/google-chrome' # Explicitly set Google Chrome binary location\n",
        "\n",
        "    # Use ChromeDriverManager to install and get the latest Chromedriver path\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "def get_job_links(driver, max_pages=20):\n",
        "    \"\"\"R√©cup√®re tous les liens d'offres SANS time.sleep\"\"\"\n",
        "    all_links = set()\n",
        "\n",
        "    for page in range(0, max_pages):\n",
        "        try:\n",
        "            url = f\"https://www.emploi.ma/recherche-jobs-maroc?page={page}\"\n",
        "            print(f\"üìÑ Scraping page {page + 1}/{max_pages}...\")\n",
        "            driver.get(url)\n",
        "\n",
        "            # Attendre intelligemment le chargement\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.CLASS_NAME, \"card-job\"))\n",
        "            )\n",
        "\n",
        "            offer_links = driver.find_elements(By.XPATH, \"//a[contains(@href, '/offre-emploi-maroc/')]\")\n",
        "\n",
        "            page_links = 0\n",
        "            for link in offer_links:\n",
        "                try:\n",
        "                    href = link.get_attribute('href')\n",
        "                    if href and '/offre-emploi-maroc/' in href:\n",
        "                        all_links.add(href)\n",
        "                        page_links += 1\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            print(f\"üîó Found {page_links} new links on page {page + 1} (Total: {len(all_links)})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error on page {page}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return list(all_links)\n",
        "\n",
        "def normalize_education_level(education_text):\n",
        "    \"\"\"Normalise le niveau d'√©tudes\"\"\"\n",
        "    if not education_text or education_text == \"N/A\":\n",
        "        return \"Non sp√©cifi√©\"\n",
        "\n",
        "    education_text = education_text.lower()\n",
        "\n",
        "    # D√©tection des niveaux d'√©tudes\n",
        "    if 'bac+5' in education_text or 'bac +5' in education_text or 'master' in education_text or 'ing√©nieur' in education_text:\n",
        "        return \"BAC+5\"\n",
        "    elif 'bac+4' in education_text or 'bac +4' in education_text or 'licence' in education_text:\n",
        "        return \"BAC+4\"\n",
        "    elif 'bac+3' in education_text or 'bac +3' in education_text:\n",
        "        return \"BAC+3\"\n",
        "    elif 'bac+2' in education_text or 'bac +2' in education_text:\n",
        "        return \"BAC+2\"\n",
        "    elif 'bac' in education_text and '+' not in education_text:\n",
        "        return \"BAC\"\n",
        "    elif 'doctorat' in education_text or 'phd' in education_text:\n",
        "        return \"DOCTORAT\"\n",
        "\n",
        "    # Si c'est une liste de niveaux\n",
        "    if any(pattern in education_text for pattern in ['bac+', 'bac +']):\n",
        "        levels_found = []\n",
        "        for level in ['bac+5', 'bac+4', 'bac+3', 'bac+2', 'bac']:\n",
        "            if level in education_text:\n",
        "                levels_found.append(level.upper())\n",
        "\n",
        "        if levels_found:\n",
        "            return \" - \".join(levels_found)\n",
        "\n",
        "    return \"Non sp√©cifi√©\"\n",
        "\n",
        "def normalize_contract_type(contract_text):\n",
        "    \"\"\"Normalise le type de contrat\"\"\"\n",
        "    if not contract_text or contract_text == \"N/A\":\n",
        "        return \"Non sp√©cifi√©\"\n",
        "\n",
        "    contract_text = contract_text.lower()\n",
        "\n",
        "    # D√©tection directe des contrats\n",
        "    if 'cdi' in contract_text:\n",
        "        return \"CDI\"\n",
        "    elif 'cdd' in contract_text:\n",
        "        return \"CDD\"\n",
        "    elif 'stage' in contract_text:\n",
        "        return \"STAGE\"\n",
        "    elif 'int√©rim' in contract_text or 'interim' in contract_text:\n",
        "        return \"INT√âRIM\"\n",
        "    elif 'freelance' in contract_text:\n",
        "        return \"FREELANCE\"\n",
        "    elif 'anapec' in contract_text:\n",
        "        return \"ANAPEC\"\n",
        "    elif 'alternance' in contract_text:\n",
        "        return \"ALTERNANCE\"\n",
        "    elif 'temps partiel' in contract_text or 'part time' in contract_text:\n",
        "        return \"TEMPS PARTIEL\"\n",
        "    elif 'mission' in contract_text:\n",
        "        return \"MISSION\"\n",
        "\n",
        "    return \"Non sp√©cifi√©\"\n",
        "\n",
        "def normalize_experience(experience_text):\n",
        "    \"\"\"Normalise l'exp√©rience requise\"\"\"\n",
        "    if not experience_text or experience_text == \"N/A\":\n",
        "        return \"Non sp√©cifi√©\"\n",
        "\n",
        "    experience_text = experience_text.lower()\n",
        "\n",
        "    # D√©tection directe\n",
        "    if 'd√©butant' in experience_text or 'junior' in experience_text or '0 an' in experience_text:\n",
        "        return \"D√©butant\"\n",
        "    elif '1 an' in experience_text or '2 ans' in experience_text:\n",
        "        return \"1-2 ans\"\n",
        "    elif '3 ans' in experience_text or '4 ans' in experience_text or '5 ans' in experience_text or 'entre 2 ans et 5 ans' in experience_text:\n",
        "        return \"2-5 ans\"\n",
        "    elif '6 ans' in experience_text or '7 ans' in experience_text or '8 ans' in experience_text or '9 ans' in experience_text or '10 ans' in experience_text or 'entre 5 ans et 10 ans' in experience_text:\n",
        "        return \"5-10 ans\"\n",
        "    elif 'plus de 10' in experience_text or '10+' in experience_text or 'senior' in experience_text:\n",
        "        return \"10+ ans\"\n",
        "\n",
        "    return \"Non sp√©cifi√©\"\n",
        "\n",
        "def scrape_job_details(driver, url):\n",
        "    \"\"\"Scrape les d√©tails d'une offre SANS time.sleep\"\"\"\n",
        "    try:\n",
        "        print(f\"    üîç Scraping: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Attendre intelligemment\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "        )\n",
        "\n",
        "        # Title\n",
        "        title = \"N/A\"\n",
        "        try:\n",
        "            title_elem = driver.find_element(By.TAG_NAME, \"h1\")\n",
        "            title = title_elem.text.strip()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Company\n",
        "        company = \"N/A\"\n",
        "        try:\n",
        "            company_elems = driver.find_elements(By.XPATH, \"//a[contains(@href, '/recruteur/')]\")\n",
        "            for elem in company_elems:\n",
        "                company_text = elem.text.strip()\n",
        "                if company_text and len(company_text) > 2 and company_text != \"Voir toutes les offres\":\n",
        "                    company = company_text\n",
        "                    break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # M√âTHODE INTELLIGENTE : Analyser TOUS les √©l√©ments\n",
        "        education_level = \"Non sp√©cifi√©\"\n",
        "        contract = \"Non sp√©cifi√©\"\n",
        "        experience = \"Non sp√©cifi√©\"\n",
        "        city = \"Non sp√©cifi√©e\"\n",
        "\n",
        "        try:\n",
        "            # R√©cup√©rer tous les √©l√©ments de la liste\n",
        "            list_items = driver.find_elements(By.XPATH, \"//*[@id='main-content']/div[2]/div[1]/div[1]/div[1]/div[1]/div/ul/li\")\n",
        "\n",
        "            for item in list_items:\n",
        "                text = item.text.strip().lower()\n",
        "\n",
        "                # D√©tection du niveau d'√©tudes\n",
        "                if any(edu_keyword in text for edu_keyword in ['bac+', 'bac +', 'dipl√¥me', 'niveau d', 'formation']):\n",
        "                    if 'exp√©rience' not in text:  # √âviter les confusions\n",
        "                        education_level = item.find_element(By.TAG_NAME, \"span\").text.strip()\n",
        "\n",
        "                # D√©tection du contrat\n",
        "                elif any(contract_keyword in text for contract_keyword in ['cdi', 'cdd', 'stage', 'int√©rim', 'contrat', 'freelance']):\n",
        "                    contract = item.find_element(By.TAG_NAME, \"span\").text.strip()\n",
        "\n",
        "                # D√©tection de l'exp√©rience\n",
        "                elif any(exp_keyword in text for exp_keyword in ['exp√©rience', 'd√©butant', 'junior', 'senior', 'ans']):\n",
        "                    if 'bac' not in text:  # √âviter les confusions\n",
        "                        experience = item.find_element(By.TAG_NAME, \"span\").text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ö†Ô∏è  M√©thode intelligente √©chou√©e: {e}\")\n",
        "\n",
        "        # VILLE - XPATH EXACT\n",
        "        try:\n",
        "            city_elem = driver.find_element(By.XPATH, \"//*[@id='main-content']/div[2]/div[1]/div[1]/div[1]/div[1]/div/ul/li[2]/span\")\n",
        "            city = city_elem.text.strip()\n",
        "        except:\n",
        "            # Fallback pour la ville\n",
        "            try:\n",
        "                location_elems = driver.find_elements(By.XPATH, \"//li[contains(., 'R√©gion') or contains(., 'Ville') or contains(., 'Lieu')]\")\n",
        "                for elem in location_elems:\n",
        "                    location_text = elem.text\n",
        "                    if \":\" in location_text:\n",
        "                        city = location_text.split(\":\")[1].strip()\n",
        "                        break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Normalisation\n",
        "        education_level = normalize_education_level(education_level)\n",
        "        contract = normalize_contract_type(contract)\n",
        "        experience = normalize_experience(experience)\n",
        "\n",
        "        print(f\"    üìå Titre: {title[:50]}...\")\n",
        "        print(f\"    üè¢ Entreprise: {company}\")\n",
        "        print(f\"    üéì Niveau: {education_level}\")\n",
        "        print(f\"    üìù Contrat: {contract}\")\n",
        "        print(f\"    üíº Exp√©rience: {experience}\")\n",
        "        print(f\"    üìç Ville: {city}\")\n",
        "\n",
        "        # Publication date\n",
        "        publication_date = \"Non sp√©cifi√©e\"\n",
        "        try:\n",
        "            date_elem = driver.find_element(By.XPATH, \"//*[@id='main-content']/div[1]/div/div[2]/p\")\n",
        "            date_text = date_elem.text.strip()\n",
        "            date_match = re.search(r'\\d{2}\\.\\d{2}\\.\\d{4}', date_text)\n",
        "            if date_match:\n",
        "                publication_date = date_match.group()\n",
        "            print(f\"    üìÖ Date: {publication_date}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Description\n",
        "        description = \"Non disponible\"\n",
        "        try:\n",
        "            desc_elems = driver.find_elements(By.XPATH, \"//div[contains(@class, 'description')] | //div[contains(@class, 'content')] | //article\")\n",
        "            for elem in desc_elems:\n",
        "                desc_text = elem.text.strip()\n",
        "                if desc_text and len(desc_text) > 50:\n",
        "                    description = re.sub(r'\\s+', ' ', desc_text)\n",
        "                    break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        job_data = {\n",
        "            \"site\": \"Emploi.ma\",\n",
        "            \"title\": title,\n",
        "            \"company\": company,\n",
        "            \"city\": city,\n",
        "            \"education_level\": education_level,\n",
        "            \"contract\": contract,\n",
        "            \"experience\": experience,\n",
        "            \"publication_date\": publication_date,\n",
        "            \"description\": description[:500] + \"...\" if len(description) > 500 else description,\n",
        "            \"url\": url\n",
        "        }\n",
        "\n",
        "        return job_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_emploi_ma():\n",
        "    \"\"\"Fonction principale de scraping TOUTES les offres\"\"\"\n",
        "    print(\"üöÄ Starting Emploi.ma scraper (TOUTES les offres)...\\n\")\n",
        "\n",
        "    driver = init_driver()\n",
        "    all_jobs = []\n",
        "\n",
        "    try:\n",
        "        # Get all job links from 20 pages\n",
        "        job_links = get_job_links(driver, max_pages=20)\n",
        "        print(f\"\\nüîó Total unique links found: {len(job_links)}\\n\")\n",
        "\n",
        "        # Scrape CHAQUE job (pas seulement 10)\n",
        "        total_links = len(job_links)\n",
        "        for i, link in enumerate(job_links, 1):\n",
        "            print(f\"[{i}/{total_links}]\")\n",
        "            job_data = scrape_job_details(driver, link)\n",
        "\n",
        "            if job_data:\n",
        "                all_jobs.append(job_data)\n",
        "                print(f\"  ‚úÖ {job_data['title'][:40]}...\")\n",
        "\n",
        "            # Afficher la progression\n",
        "            if i % 10 == 0:\n",
        "                print(f\"\\nüìä Progression: {i}/{total_links} offres trait√©es ({i/total_links*100:.1f}%)\\n\")\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    return all_jobs\n",
        "\n",
        "# EX√âCUTION\n",
        "if __name__ == \"__main__\":\n",
        "    jobs = scrape_emploi_ma()\n",
        "\n",
        "    if jobs:\n",
        "        df = pd.DataFrame(jobs)\n",
        "\n",
        "        # Renommer les colonnes en fran√ßais\n",
        "        df = df.rename(columns={\n",
        "            \"site\": \"site\",\n",
        "            \"title\": \"intitul√©_poste\",\n",
        "            \"company\": \"entreprise\",\n",
        "            \"city\": \"ville\",\n",
        "            \"education_level\": \"niveau_√©tudes\",\n",
        "            \"contract\": \"type_contrat\",\n",
        "            \"experience\": \"exp√©rience\",\n",
        "            \"publication_date\": \"publication_date\",\n",
        "            \"description\": \"description\",\n",
        "            \"url\": \"lien_offre\"\n",
        "        })\n",
        "\n",
        "        filename = \"emploi_ma_toutes_offres.csv\"\n",
        "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"\\nüéâ COLLECTE TERMIN√âE!\")\n",
        "        print(f\"üìÅ Fichier sauvegard√©: {filename}\")\n",
        "        print(f\"üìä Total offres collect√©es: {len(df)}\")\n",
        "\n",
        "        # Rapport d√©taill√©\n",
        "        print(f\"\\nüìà RAPPORT STATISTIQUE:\")\n",
        "        print(f\"‚Ä¢ Villes: {df['ville'].value_counts().head(10).to_dict()}\")\n",
        "        print(f\"‚Ä¢ Types de contrat: {df['type_contrat'].value_counts().to_dict()}\")\n",
        "        print(f\"‚Ä¢ Niveaux d'√©tudes: {df['niveau_√©tudes'].value_counts().to_dict()}\")\n",
        "        print(f\"‚Ä¢ Exp√©riences: {df['exp√©rience'].value_counts().to_dict()}\")\n",
        "\n",
        "        # Aper√ßu des donn√©es\n",
        "        print(f\"\\nüëÄ APER√áU DES DONN√âES:\")\n",
        "        print(df[['intitul√©_poste', 'entreprise', 'ville', 'niveau_√©tudes', 'type_contrat']].head(15))\n",
        "    else:\n",
        "        print(\"‚ùå No jobs scraped!\")"
      ],
      "metadata": {
        "id": "Fkg7WJElrZ3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "AZK6wF6svNl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "CgpWEdm1vS1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rekrute Scraper**"
      ],
      "metadata": {
        "id": "SSi--gGRwp0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "all_jobs = []\n",
        "scraped_urls = set()\n",
        "\n",
        "def normalize_url(url):\n",
        "    \"\"\"Normalise l'URL pour √©viter les doublons\"\"\"\n",
        "    url = url.split('#')[0].split('?')[0]\n",
        "    return url.rstrip('/')\n",
        "\n",
        "def get_job_links(page_number):\n",
        "    url = f\"https://www.rekrute.com/offres.html?s=1&p={page_number}&o=1\"\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        links = []\n",
        "\n",
        "        for a in soup.select(\"a[href*='offre-emploi']\"):\n",
        "            href = a.get(\"href\", \"\")\n",
        "            if \"offre-emploi\" in href:\n",
        "                if href.startswith(\"http\"):\n",
        "                    full_url = href\n",
        "                elif href.startswith(\"/\"):\n",
        "                    full_url = \"https://www.rekrute.com\" + href\n",
        "                else:\n",
        "                    full_url = \"https://www.rekrute.com/\" + href\n",
        "\n",
        "                normalized_url = normalize_url(full_url)\n",
        "\n",
        "                if normalized_url not in scraped_urls:\n",
        "                    links.append(normalized_url)\n",
        "                    scraped_urls.add(normalized_url)\n",
        "\n",
        "        return links\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error fetching page {page_number}: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_company(soup):\n",
        "    \"\"\"Extraire le nom de l'entreprise\"\"\"\n",
        "    logo_img = soup.find(\"img\", class_=\"photo\")\n",
        "    if logo_img:\n",
        "        company_name = logo_img.get(\"alt\") or logo_img.get(\"title\")\n",
        "        if company_name and company_name.strip():\n",
        "            return company_name.strip()\n",
        "\n",
        "    company_link = soup.find(\"a\", href=re.compile(r\"-emploi-recrutement-\\d+\\.html\"))\n",
        "    if company_link:\n",
        "        href = company_link.get(\"href\", \"\")\n",
        "        match = re.search(r'/([a-z0-9-]+)-emploi-recrutement-\\d+\\.html', href)\n",
        "        if match:\n",
        "            company_slug = match.group(1)\n",
        "            company_name = company_slug.replace('-', ' ').title()\n",
        "            return company_name\n",
        "\n",
        "    for div in soup.find_all(\"div\", class_=re.compile(r\"company|recruiter\")):\n",
        "        text = div.get_text(strip=True)\n",
        "        if text and 3 < len(text) < 100:\n",
        "            if '.' not in text[:50]:\n",
        "                return text\n",
        "\n",
        "    return \"Confidentiel\"\n",
        "\n",
        "def extract_education_level(soup):\n",
        "    \"\"\"Extraire le niveau d'√©tudes\"\"\"\n",
        "    education_keywords = {\n",
        "        'BAC+5': ['bac+5', 'bac +5', 'master', 'ing√©nieur', 'engineer', 'sup√©rieur'],\n",
        "        'BAC+4': ['bac+4', 'bac +4', 'licence', 'bachelor'],\n",
        "        'BAC+3': ['bac+3', 'bac +3', 'technicien', 'dut', 'deug'],\n",
        "        'BAC+2': ['bac+2', 'bac +2', 'deust', 'ts'],\n",
        "        'BAC': ['bac', 'baccalaur√©at'],\n",
        "        'DOCTORAT': ['doctorat', 'phd', 'doctorant']\n",
        "    }\n",
        "\n",
        "    full_text = soup.get_text().lower()\n",
        "\n",
        "    for level, keywords in education_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword in full_text:\n",
        "                return level\n",
        "\n",
        "    for tag in soup.find_all([\"li\", \"span\", \"div\"]):\n",
        "        text = tag.get_text().lower()\n",
        "        if any(word in text for word in ['bac', 'niveau', 'formation', 'dipl√¥me', '√©tude']):\n",
        "            for level, keywords in education_keywords.items():\n",
        "                for keyword in keywords:\n",
        "                    if keyword in text:\n",
        "                        return level\n",
        "\n",
        "    return \"Non sp√©cifi√©\"\n",
        "\n",
        "def extract_contract_type(soup):\n",
        "    \"\"\"Extraire le type de contrat\"\"\"\n",
        "    contract_types = {\n",
        "        'CDI': ['cdi', 'contrat √† dur√©e ind√©termin√©e'],\n",
        "        'CDD': ['cdd', 'contrat √† dur√©e d√©termin√©e'],\n",
        "        'STAGE': ['stage', 'internship', 'stagiaire'],\n",
        "        'FREELANCE': ['freelance', 'ind√©pendant', 'consultant'],\n",
        "        'ANAPEC': ['anapec', 'contrat anapec'],\n",
        "        'ALTERNANCE': ['alternance', 'apprentissage'],\n",
        "        'TEMPS PARTIEL': ['temps partiel', 'part time'],\n",
        "        'MISSION': ['mission', 'contrat de mission']\n",
        "    }\n",
        "\n",
        "    full_text = soup.get_text().lower()\n",
        "\n",
        "    for span in soup.find_all(\"span\", class_=re.compile(\"tagContract|contrat|type\")):\n",
        "        text = span.get_text().lower()\n",
        "        for contract, keywords in contract_types.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in text:\n",
        "                    return contract\n",
        "\n",
        "    for contract, keywords in contract_types.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword in full_text:\n",
        "                return contract\n",
        "\n",
        "    return \"Non sp√©cifi√©\"\n",
        "\n",
        "def extract_experience(soup):\n",
        "    \"\"\"Extraire l'exp√©rience requise\"\"\"\n",
        "    experience_patterns = [\n",
        "        (r'(\\d+)\\s*ans?', lambda x: f\"{x} ans\"),\n",
        "        (r'(\\d+)\\s*√†\\s*(\\d+)\\s*ans?', lambda x, y: f\"{x}-{y} ans\"),\n",
        "        (r'd√©butant|junior|entry level', \"D√©butant\"),\n",
        "        (r'senior|exp√©riment√©|experimente', \"Senior\"),\n",
        "        (r'(\\d+)\\s*ann√©es?', lambda x: f\"{x} ans\"),\n",
        "        (r'(\\d+)\\s*ans?', lambda x: f\"{x} ans\")\n",
        "    ]\n",
        "\n",
        "    for tag in soup.find_all([\"li\", \"span\", \"div\"]):\n",
        "        text = tag.get_text().lower()\n",
        "        if any(word in text for word in ['exp√©rience', 'experience', 'ann√©es', 'ans']):\n",
        "            for pattern, converter in experience_patterns:\n",
        "                matches = re.search(pattern, text)\n",
        "                if matches:\n",
        "                    if callable(converter):\n",
        "                        return converter(*matches.groups())\n",
        "                    else:\n",
        "                        return converter\n",
        "\n",
        "    full_text = soup.get_text().lower()\n",
        "    for pattern, converter in experience_patterns:\n",
        "        matches = re.search(pattern, full_text)\n",
        "        if matches:\n",
        "            if callable(converter):\n",
        "                return converter(*matches.groups())\n",
        "            else:\n",
        "                return converter\n",
        "\n",
        "    return \"Non sp√©cifi√©\"\n",
        "\n",
        "def extract_publication_date(soup):\n",
        "    \"\"\"Extraire la VRAIE date de publication - VERSION FINALE\"\"\"\n",
        "\n",
        "    print(\"    üîç Recherche de la date de publication...\")\n",
        "\n",
        "    # METHODE 1: Chercher \"Publi√©e aujourd'hui\"\n",
        "    full_text = soup.get_text()\n",
        "\n",
        "    if \"publi√©e aujourd'hui\" in full_text.lower():\n",
        "        date_publication = datetime.now().strftime(\"%d/%m/%Y\")\n",
        "        print(f\"    üìÖ Trouv√©e: Publi√©e aujourd'hui ‚Üí {date_publication}\")\n",
        "        return date_publication\n",
        "\n",
        "    # METHODE 2: Chercher \"Publi√©e il y a X jours\"\n",
        "    jours_match = re.search(r'publi√©e il y a (\\d+)\\s*jours?', full_text, re.IGNORECASE)\n",
        "    if jours_match:\n",
        "        jours = int(jours_match.group(1))\n",
        "        date_publication = (datetime.now() - timedelta(days=jours)).strftime(\"%d/%m/%Y\")\n",
        "        print(f\"    üìÖ Calcul√©e: Publi√©e il y a {jours} jours ‚Üí {date_publication}\")\n",
        "        return date_publication\n",
        "\n",
        "    # METHODE 3: Chercher \"Publi√©e il y a X heures\"\n",
        "    heures_match = re.search(r'publi√©e il y a (\\d+)\\s*heures?', full_text, re.IGNORECASE)\n",
        "    if heures_match:\n",
        "        date_publication = datetime.now().strftime(\"%d/%m/%Y\")\n",
        "        print(f\"    üìÖ Calcul√©e: Publi√©e il y a {heures_match.group(1)} heures ‚Üí {date_publication}\")\n",
        "        return date_publication\n",
        "\n",
        "    # METHODE 4: Chercher dans <em class=\"date\"> qui contient \"Publication : du X au Y\"\n",
        "    date_em = soup.find(\"em\", class_=\"date\")\n",
        "    if date_em:\n",
        "        em_text = date_em.get_text()\n",
        "        print(f\"    üìù Contenu <em class='date'>: {em_text[:150]}...\")\n",
        "\n",
        "        # Pattern: \"Publication : du DD/MM/YYYY au DD/MM/YYYY\"\n",
        "        pub_match = re.search(r'Publication\\s*:\\s*du\\s*(\\d{2}/\\d{2}/\\d{4})\\s*au\\s*(\\d{2}/\\d{2}/\\d{4})', em_text, re.IGNORECASE)\n",
        "        if pub_match:\n",
        "            date_debut = pub_match.group(1)\n",
        "            date_fin = pub_match.group(2)\n",
        "            print(f\"    üìÖ Trouv√©e (format complet): du {date_debut} au {date_fin}\")\n",
        "            print(f\"    ‚úÖ Date de publication retenue: {date_debut}\")\n",
        "            return date_debut\n",
        "\n",
        "        # Pattern: \"Publication : du DD/MM/YYYY\"\n",
        "        pub_match_simple = re.search(r'Publication\\s*:\\s*du\\s*(\\d{2}/\\d{2}/\\d{4})', em_text, re.IGNORECASE)\n",
        "        if pub_match_simple:\n",
        "            date_publication = pub_match_simple.group(1)\n",
        "            print(f\"    üìÖ Trouv√©e (format standard): {date_publication}\")\n",
        "            return date_publication\n",
        "\n",
        "    # METHODE 5: Chercher \"Publication : du DATE au DATE\" dans tout le texte\n",
        "    pub_match_full = re.search(r'Publication\\s*:\\s*du\\s*(\\d{2}/\\d{2}/\\d{4})\\s*au\\s*(\\d{2}/\\d{2}/\\d{4})', full_text, re.IGNORECASE)\n",
        "    if pub_match_full:\n",
        "        date_debut = pub_match_full.group(1)\n",
        "        date_fin = pub_match_full.group(2)\n",
        "        print(f\"    üìÖ Trouv√©e dans texte: du {date_debut} au {date_fin}\")\n",
        "        print(f\"    ‚úÖ Date de publication retenue: {date_debut}\")\n",
        "        return date_debut\n",
        "\n",
        "    # Pattern: \"Publication : du DD/MM/YYYY\"\n",
        "    pub_match_simple_full = re.search(r'Publication\\s*:\\s*du\\s*(\\d{2}/\\d{2}/\\d{4})', full_text, re.IGNORECASE)\n",
        "    if pub_match_simple_full:\n",
        "        date_publication = pub_match_simple_full.group(1)\n",
        "        print(f\"    üìÖ Trouv√©e dans texte: {date_publication}\")\n",
        "        return date_publication\n",
        "\n",
        "    # METHODE 3: Chercher dans toutes les balises <span> pour les dates\n",
        "    # Le XPath //*[@id=\"175288\"]/div/div[2]/div/div/em/span[1] sugg√®re span[1]\n",
        "    all_spans = soup.find_all(\"span\")\n",
        "    for span in all_spans:\n",
        "        span_text = span.get_text()\n",
        "        # Chercher une date DD/MM/YYYY dans ce span\n",
        "        date_match = re.search(r'(\\d{2}/\\d{2}/\\d{4})', span_text)\n",
        "        if date_match:\n",
        "            date_found = date_match.group(1)\n",
        "            # V√©rifier si c'est une date valide (dans le pass√© r√©cent)\n",
        "            try:\n",
        "                date_obj = datetime.strptime(date_found, \"%d/%m/%Y\")\n",
        "                today = datetime.now()\n",
        "                days_diff = (today - date_obj).days\n",
        "\n",
        "                if 0 <= days_diff <= 180:  # Dans les 6 derniers mois\n",
        "                    # V√©rifier le contexte pour confirmer que c'est bien la date de publication\n",
        "                    parent_text = span.parent.get_text() if span.parent else \"\"\n",
        "                    if \"publication\" in parent_text.lower() or \"publi\" in parent_text.lower():\n",
        "                        print(f\"    üìÖ Trouv√©e dans <span> (contexte publication): {date_found}\")\n",
        "                        return date_found\n",
        "            except:\n",
        "                continue\n",
        "    date_em = soup.find(\"em\", class_=\"date\")\n",
        "    if date_em:\n",
        "        em_text = date_em.get_text()\n",
        "        print(f\"    üìù Contenu <em class='date'>: {em_text}\")\n",
        "\n",
        "        # Extraire \"du XX/XX/XXXX\"\n",
        "        date_match = re.search(r'du\\s*(\\d{2}/\\d{2}/\\d{4})', em_text)\n",
        "        if date_match:\n",
        "            date_publication = date_match.group(1)\n",
        "            print(f\"    üìÖ Trouv√©e dans <em>: {date_publication}\")\n",
        "            return date_publication\n",
        "\n",
        "    # METHODE 4: Chercher \"Publi√©e il y a X jours\" et CALCULER la date\n",
        "    jours_match = re.search(r'publi√©e il y a (\\d+)\\s*jours?', full_text, re.IGNORECASE)\n",
        "    if jours_match:\n",
        "        jours = int(jours_match.group(1))\n",
        "        date_publication = (datetime.now() - timedelta(days=jours)).strftime(\"%d/%m/%Y\")\n",
        "        print(f\"    üìÖ Calcul√©e: Publi√©e il y a {jours} jours ‚Üí {date_publication}\")\n",
        "        return date_publication\n",
        "\n",
        "    # METHODE 5: Chercher \"Publi√©e il y a X heures\"\n",
        "    heures_match = re.search(r'publi√©e il y a (\\d+)\\s*heures?', full_text, re.IGNORECASE)\n",
        "    if heures_match:\n",
        "        date_publication = datetime.now().strftime(\"%d/%m/%Y\")\n",
        "        print(f\"    üìÖ Calcul√©e: Publi√©e aujourd'hui\")\n",
        "        return date_publication\n",
        "\n",
        "    # METHODE 6: Chercher directement les dates au format DD/MM/YYYY\n",
        "    # IMPORTANT: On cherche la date de D√âBUT, pas la date limite\n",
        "    dates = re.findall(r'\\b(\\d{2}/\\d{2}/\\d{4})\\b', full_text)\n",
        "    if dates:\n",
        "        today = datetime.now()\n",
        "        valid_dates = []\n",
        "\n",
        "        for date_str in dates:\n",
        "            try:\n",
        "                date_obj = datetime.strptime(date_str, \"%d/%m/%Y\")\n",
        "                days_diff = (today - date_obj).days\n",
        "\n",
        "                # Crit√®res de validation :\n",
        "                # - Date dans le pass√© (days_diff >= 0)\n",
        "                # - Pas trop ancienne (moins de 6 mois = 180 jours)\n",
        "                # - Pas dans le futur\n",
        "                if 0 <= days_diff <= 180:\n",
        "                    valid_dates.append((date_str, days_diff))\n",
        "                    print(f\"    ‚úì Date valide trouv√©e: {date_str} (il y a {days_diff} jours)\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Si on a des dates valides, prendre la PLUS R√âCENTE (la plus proche d'aujourd'hui)\n",
        "        if valid_dates:\n",
        "            # Trier par nombre de jours (du plus r√©cent au plus ancien)\n",
        "            valid_dates.sort(key=lambda x: x[1])\n",
        "            best_date = valid_dates[0][0]\n",
        "            print(f\"    üìÖ Date de publication retenue: {best_date}\")\n",
        "            return best_date\n",
        "\n",
        "        # Si aucune date valide, indiquer clairement qu'on ne peut pas d√©terminer\n",
        "        print(f\"    ‚ö†Ô∏è  Dates trouv√©es ({len(dates)}) mais aucune dans le pass√© r√©cent\")\n",
        "        print(f\"    Dates d√©tect√©es: {dates[:3]}\")  # Afficher les 3 premi√®res\n",
        "\n",
        "    print(\"    ‚ùå Aucune information de date trouv√©e\")\n",
        "    return \"Non sp√©cifi√©e\"\n",
        "\n",
        "def extract_city(soup):\n",
        "    \"\"\"Extraire la ville\"\"\"\n",
        "    title_tag = soup.select_one(\"h1\")\n",
        "    if title_tag:\n",
        "        title = title_tag.text.strip()\n",
        "        if \" - \" in title:\n",
        "            parts = title.split(\" - \")\n",
        "            if len(parts) > 1:\n",
        "                city_candidate = parts[-1].strip()\n",
        "                villes_maroc = ['casablanca', 'rabat', 'marrakech', 'tanger', 'f√®s', 'mekn√®s',\n",
        "                               'agadir', 'kenitra', 'oujda', 'tetouan', 'sal√©', 'mohammedia',\n",
        "                               'el jadida', 'safi', 'beni mellal', 'nador', 't√©touan']\n",
        "                if any(ville in city_candidate.lower() for ville in villes_maroc):\n",
        "                    return city_candidate\n",
        "\n",
        "    city_selectors = [\n",
        "        \"span.city\",\n",
        "        \"div.location\",\n",
        "        \"li.location\",\n",
        "        \"span[title*='Region']\",\n",
        "        \"span[title*='Ville']\"\n",
        "    ]\n",
        "\n",
        "    for selector in city_selectors:\n",
        "        city_tag = soup.select_one(selector)\n",
        "        if city_tag:\n",
        "            city = city_tag.text.strip()\n",
        "            if city and city != \"Non sp√©cifi√©e\":\n",
        "                return city\n",
        "\n",
        "    return \"Non sp√©cifi√©e\"\n",
        "\n",
        "def extract_description(soup):\n",
        "    \"\"\"Extraire la description\"\"\"\n",
        "    selectors = [\n",
        "        \"div.post-content\",\n",
        "        \"div.description\",\n",
        "        \"div.job-description\",\n",
        "        \"div.details-post\",\n",
        "        \"div#recruiterDescription p\"\n",
        "    ]\n",
        "\n",
        "    for selector in selectors:\n",
        "        desc = soup.select_one(selector)\n",
        "        if desc:\n",
        "            text = desc.get_text(strip=True)\n",
        "            if len(text) > 50:\n",
        "                text = re.sub(r'\\s+', ' ', text)\n",
        "                return text[:500]\n",
        "\n",
        "    return \"Description non disponible\"\n",
        "\n",
        "def scrape_job_details(url):\n",
        "    try:\n",
        "        print(f\"    üîç Scraping: {url}\")\n",
        "        r = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        # Title\n",
        "        title_tag = soup.select_one(\"h1\")\n",
        "        title = title_tag.text.strip() if title_tag else \"N/A\"\n",
        "\n",
        "        # Company\n",
        "        company = extract_company(soup)\n",
        "\n",
        "        # City\n",
        "        city = extract_city(soup)\n",
        "\n",
        "        # Niveau d'√©tudes\n",
        "        education = extract_education_level(soup)\n",
        "\n",
        "        # Type de contrat\n",
        "        contract = extract_contract_type(soup)\n",
        "\n",
        "        # Exp√©rience\n",
        "        experience = extract_experience(soup)\n",
        "\n",
        "        # Date de publication - CORRIG√â\n",
        "        publication = extract_publication_date(soup)\n",
        "\n",
        "        # Description\n",
        "        description = extract_description(soup)\n",
        "\n",
        "        job_data = {\n",
        "            \"site\": \"Rekrute\",\n",
        "            \"title\": title,\n",
        "            \"company\": company,\n",
        "            \"city\": city,\n",
        "            \"niveau_etudes\": education,\n",
        "            \"contract\": contract,\n",
        "            \"experience\": experience,\n",
        "            \"publication_date\": publication,\n",
        "            \"description\": description,\n",
        "            \"url\": url\n",
        "        }\n",
        "\n",
        "        return job_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main scraping loop\n",
        "print(\"üöÄ Starting Rekrute scraper...\\n\")\n",
        "for page in range(1, 80):\n",
        "    print(f\"üìÑ Scraping page {page}...\")\n",
        "    links = get_job_links(page)\n",
        "    print(f\"üîó Found {len(links)} new job links\")\n",
        "\n",
        "    for link in links:\n",
        "        job = scrape_job_details(link)\n",
        "        if job:\n",
        "            all_jobs.append(job)\n",
        "            print(f\"  ‚úÖ {job['company']} - {job['title'][:40]}...\")\n",
        "            print(f\"     üìÖ {job['publication_date']} | üìç {job['city']}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# Save to CSV\n",
        "if all_jobs:\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "\n",
        "    # Supprimer les doublons\n",
        "    df['url_normalized'] = df['url'].apply(normalize_url)\n",
        "    df = df.drop_duplicates(subset=['url_normalized'], keep='first')\n",
        "    df = df.drop(columns=['url_normalized'])\n",
        "\n",
        "    # Renommer les colonnes en fran√ßais\n",
        "    df = df.rename(columns={\n",
        "        \"site\": \"site\",\n",
        "        \"title\": \"intitul√©_poste\",\n",
        "        \"company\": \"entreprise\",\n",
        "        \"city\": \"ville\",\n",
        "        \"niveau_etudes\": \"niveau_√©tudes\",\n",
        "        \"contract\": \"type_contrat\",\n",
        "        \"experience\": \"exp√©rience\",\n",
        "        \"publication_date\": \"publication_date\",\n",
        "        \"description\": \"description\",\n",
        "        \"url\": \"lien_offre\"\n",
        "    })\n",
        "\n",
        "    # Sauvegarder\n",
        "    df.to_csv(\"rekrute_jobs_final.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"\\nüéâ COLLECTE TERMIN√âE!\")\n",
        "    print(f\"üìä Total offres collect√©es: {len(df)}\")\n",
        "\n",
        "    # Rapport d√©taill√©\n",
        "    print(f\"\\nüìà RAPPORT STATISTIQUE:\")\n",
        "    print(f\"‚Ä¢ Entreprises: {df['entreprise'].value_counts().head(5).to_dict()}\")\n",
        "    print(f\"‚Ä¢ Contrats: {df['type_contrat'].value_counts().to_dict()}\")\n",
        "    print(f\"‚Ä¢ Niveaux √©tudes: {df['niveau_√©tudes'].value_counts().to_dict()}\")\n",
        "    print(f\"‚Ä¢ Villes: {df['ville'].value_counts().head(5).to_dict()}\")\n",
        "    print(f\"‚Ä¢ Dates publication: {df['publication_date'].value_counts().head(10).to_dict()}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Aucune offre trouv√©e\")"
      ],
      "metadata": {
        "id": "AFQbAp0Awitd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "lHZjZisySd0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "1DgHOrXLRAdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Collection**"
      ],
      "metadata": {
        "id": "yIrDNyAoSrqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def simple_merge_csv_to_json():\n",
        "    \"\"\"Version ultra-simplifi√©e pour fusionner deux CSV en JSON\"\"\"\n",
        "\n",
        "    # Lire les deux fichiers CSV\n",
        "    df_rekrute = pd.read_csv(\"rekrute_jobs_final.csv\", encoding='utf-8-sig')\n",
        "    df_emploi_ma = pd.read_csv(\"emploi_ma_toutes_offres.csv\", encoding='utf-8-sig')\n",
        "\n",
        "    # Ajouter la colonne 'site' si elle n'existe pas\n",
        "    if 'site' not in df_rekrute.columns:\n",
        "        df_rekrute['site'] = 'ReKrute'\n",
        "    if 'site' not in df_emploi_ma.columns:\n",
        "        df_emploi_ma['site'] = 'Emploi_ma'\n",
        "\n",
        "    # Fusionner les DataFrames\n",
        "    df_combined = pd.concat([df_rekrute, df_emploi_ma], ignore_index=True)\n",
        "\n",
        "    # Cr√©er la structure JSON\n",
        "    output_data = {\n",
        "        \"metadata\": {\n",
        "            \"date_export\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"total_offres\": len(df_combined),\n",
        "            \"sites_scrapes\": [\"ReKrute\", \"Emploi_ma\"]\n",
        "        },\n",
        "        \"offres_emploi\": df_combined.to_dict('records')\n",
        "    }\n",
        "\n",
        "    # Sauvegarder en JSON\n",
        "    with open(\"tous_les_jobs_maroc.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Fusion r√©ussie! {len(df_combined)} offres sauvegard√©es dans tous_les_jobs_maroc.json\")\n",
        "\n",
        "# Ex√©cuter\n",
        "if __name__ == \"__main__\":\n",
        "    simple_merge_csv_to_json()"
      ],
      "metadata": {
        "id": "GSkjAg3aScXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"Monter Google Drive\"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mont√© avec succ√®s!\")\n",
        "\n",
        "def simple_merge_csv_to_json_with_drive():\n",
        "    \"\"\"Fusionner CSV et sauvegarder dans Google Drive\"\"\"\n",
        "\n",
        "    # Monter Google Drive\n",
        "    mount_google_drive()\n",
        "\n",
        "    # Lire les deux fichiers CSV\n",
        "    df_rekrute = pd.read_csv(\"rekrute_jobs_final.csv\", encoding='utf-8-sig')\n",
        "    df_emploi_ma = pd.read_csv(\"emploi_ma_toutes_offres.csv\", encoding='utf-8-sig')\n",
        "\n",
        "    # Ajouter la colonne 'site' si elle n'existe pas\n",
        "    if 'site' not in df_rekrute.columns:\n",
        "        df_rekrute['site'] = 'ReKrute'\n",
        "    if 'site' not in df_emploi_ma.columns:\n",
        "        df_emploi_ma['site'] = 'Emploi_ma'\n",
        "\n",
        "    # Fusionner les DataFrames\n",
        "    df_combined = pd.concat([df_rekrute, df_emploi_ma], ignore_index=True)\n",
        "\n",
        "    # Cr√©er la structure JSON\n",
        "    output_data = {\n",
        "        \"metadata\": {\n",
        "            \"date_export\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"total_offres\": len(df_combined),\n",
        "            \"sites_scrapes\": [\"ReKrute\", \"Emploi_ma\"]\n",
        "        },\n",
        "        \"offres_emploi\": df_combined.to_dict('records')\n",
        "    }\n",
        "\n",
        "    # Sauvegarder en JSON localement\n",
        "    local_filename = \"tous_les_jobs_maroc.json\"\n",
        "    with open(local_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Sauvegarder dans Google Drive\n",
        "    drive_path = \"/content/drive/MyDrive/tous_les_jobs_maroc.json\"\n",
        "    with open(drive_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Fusion r√©ussie! {len(df_combined)} offres sauvegard√©es\")\n",
        "    print(f\"üìÅ Fichier local: {local_filename}\")\n",
        "    print(f\"üìÅ Fichier Drive: {drive_path}\")\n",
        "\n",
        "# Fonction pour afficher les donn√©es\n",
        "def display_json_data(json_file=\"tous_les_jobs_maroc.json\"):\n",
        "    # Charger le JSON\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Afficher les m√©tadonn√©es\n",
        "    print(\"üìä M√âTADONN√âES:\")\n",
        "    print(f\"‚Ä¢ Date d'export: {data['metadata']['date_export']}\")\n",
        "    print(f\"‚Ä¢ Total offres: {data['metadata']['total_offres']}\")\n",
        "    print(f\"‚Ä¢ Sites scrap√©s: {', '.join(data['metadata']['sites_scrapes'])}\")\n",
        "\n",
        "    # Convertir en DataFrame pour un affichage tabulaire\n",
        "    df = pd.DataFrame(data['offres_emploi'])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üëÄ APER√áU DES DONN√âES (√©quivalent de df.head()):\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Afficher les premi√®res lignes\n",
        "    print(f\"\\nüìã Premi√®res {min(10, len(df))} offres:\")\n",
        "    print(df.head(10).to_string(max_colwidth=30))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Ex√©cuter\n",
        "if __name__ == \"__main__\":\n",
        "    simple_merge_csv_to_json_with_drive()\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    df = display_json_data()"
      ],
      "metadata": {
        "id": "UxvNSYUWpCBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PR√âTRAITEMENT**"
      ],
      "metadata": {
        "id": "cEEJ_f6uFxge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pr√©traitement Simple**"
      ],
      "metadata": {
        "id": "AOTmNgBPGJc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "23ry9P82GI2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Charger le JSON\n",
        "file_path = \"/content/drive/MyDrive/tous_les_jobs_maroc.json\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Transformer en DataFrame\n",
        "df = pd.json_normalize(data[\"offres_emploi\"])\n",
        "\n",
        "# Inspecter les donn√©es\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "QJe36vMnGbXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Combiner titre + description\n",
        "df[\"text\"] = df[\"intitul√©_poste\"].fillna(\"\") + \" \" + df[\"description\"].fillna(\"\")\n",
        "\n",
        "# Nettoyage l√©ger\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)      # URLs\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)             # emails\n",
        "    text = re.sub(r'[^\\w\\s\\-\\+\\.,;:√†√¢√ß√©√®√™√´√Æ√Ø√¥√ª√π√º√ø√±√¶≈ì]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()       # espaces multiples\n",
        "    return text\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# Supprimer les doublons et les valeurs manquantes\n",
        "df.dropna(subset=[\"clean_text\"], inplace=True)\n",
        "df.drop_duplicates(subset=[\"clean_text\"], inplace=True)"
      ],
      "metadata": {
        "id": "H2HoGsBOGfCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspecter les donn√©es\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "Tr3k8KLTGju-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pr√©traitement total**\n",
        "\n",
        "IMPORTANT : on ne touche jamais aux colonnes utiles pour l‚Äôaffichage (site, lien_offre, ville, entreprise). On ne pr√©traite que le texte utilis√© pour l‚Äôentra√Ænement NLP.\n",
        "\n",
        "‚û°Ô∏èOn garde les colonnes brutes, on cr√©e une colonne pr√©trait√©e."
      ],
      "metadata": {
        "id": "sjLcl5tyGodG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Lemmatisation et suppression stopwords (optionnel pour NLP classique)\n",
        "#\n",
        "import spacy\n",
        "!python -m spacy download fr_core_news_sm\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "    for token in doc:      #spaCy d√©coupe le texte en tokens\n",
        "        if token.is_stop or token.is_punct or token.like_num:   #enl√®ver les stopwords ‚Äúet‚Äù, ‚Äúde‚Äù, ‚Äúle‚Äù, ‚Äúla‚Äù, ‚Äúun‚Äù‚Ä¶\n",
        "                 continue\n",
        "        lemma = token.lemma_.strip()\n",
        "        if len(lemma) > 2:\n",
        "            tokens.append(lemma)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"text_clean\"] = df[\"clean_text\"].apply(preprocess_spacy)"
      ],
      "metadata": {
        "id": "_hqmgLqXGlbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 Garder les colonnes originales intactes\n",
        "# Colonnes utiles pour l‚Äôaffichage :\n",
        "# site, lien_offre, entreprise, ville, type_contrat, niveau_√©tudes, publication_date\n",
        "columns_to_keep = [\"site\", \"intitul√©_poste\", \"entreprise\", \"ville\",\n",
        "                   \"niveau_√©tudes\", \"type_contrat\", \"exp√©rience\",\n",
        "                   \"publication_date\", \"description\", \"lien_offre\",\n",
        "                   \"text_clean\"]\n",
        "\n",
        "df_final = df[columns_to_keep]"
      ],
      "metadata": {
        "id": "WHPeBAM9G9RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_final.head())\n",
        "print(df_final.info())"
      ],
      "metadata": {
        "id": "onSgk9XYG_4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"text_clean\"].head())"
      ],
      "metadata": {
        "id": "DNSAwU-mHDkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_path = \"/content/drive/MyDrive/tous_les_jobs_maroc_pretraite.csv\"\n",
        "df_final.to_csv(output_file_path, index=False, encoding='utf-8')\n",
        "print(f\"Fichier enregistr√© avec succ√®s √† : {output_file_path}\")"
      ],
      "metadata": {
        "id": "f3IosWqpHFC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Traitement**"
      ],
      "metadata": {
        "id": "dArZo9vdnEGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Installation de biblioth√®que SentenceTransformer**"
      ],
      "metadata": {
        "id": "WzU0t9O3oPWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "K53xwV1lnHZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importation des librairies n√©cessaires**"
      ],
      "metadata": {
        "id": "Nq5XHmJboULq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "KjQWDyuxnNW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Chargement du CSV pr√©trait√©**"
      ],
      "metadata": {
        "id": "9xMNkTMWokem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.read_csv(\"/content/drive/MyDrive/tous_les_jobs_maroc_pretraite.csv\")"
      ],
      "metadata": {
        "id": "fsnKV37JnQvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final[[\"intitul√©_poste\", \"text_clean\"]].head()"
      ],
      "metadata": {
        "id": "7_h9RztnnR38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Chargement de mod√®le d'embeddings**"
      ],
      "metadata": {
        "id": "ZfuXvYYSowG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "OzXCFacBnUun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Transformation : les descriptions en embeddings**"
      ],
      "metadata": {
        "id": "X8Z4uhsKo3lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_jobs = model.encode(\n",
        "    df_final[\"text_clean\"].tolist(),\n",
        "    convert_to_tensor=False,   # numpy array\n",
        "    show_progress_bar=True\n",
        ")"
      ],
      "metadata": {
        "id": "VFfKcKpGnbCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sauvegardage des embeddings**"
      ],
      "metadata": {
        "id": "xzao8nL4pAUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/drive/MyDrive/embeddings_jobs.npy\", embeddings_jobs)\n",
        "\n",
        "print(\"‚û°Ô∏è Embeddings sauvegard√©s avec succ√®s !\")"
      ],
      "metadata": {
        "id": "m8ljyIFhpGtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape des embeddings :\", np.array(embeddings_jobs).shape)"
      ],
      "metadata": {
        "id": "ILzgvkTnpLgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_path = \"/content/drive/MyDrive/tous_les_jobs_maroc_pretraite.csv\"\n",
        "df_final.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Fichier enregistr√© avec succ√®s √† : {output_file_path}\")"
      ],
      "metadata": {
        "id": "9BaWSBvopN0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "s7S0m33OpxPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 1) Charger le CSV pr√©trait√©\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/tous_les_jobs_maroc_pretraite.csv\")\n",
        "print(\"‚úîÔ∏è Dataset charg√© :\", df.shape)\n",
        "\n",
        "# 2) Charger les embeddings sauvegard√©s\n",
        "embeddings_jobs = np.load(\"/content/drive/MyDrive/embeddings_jobs.npy\")\n",
        "print(\"‚úîÔ∏è Embeddings charg√©s :\", embeddings_jobs.shape)\n",
        "\n",
        "# 3) Convertir vers Tensor PyTorch\n",
        "embeddings_jobs_tensor = torch.from_numpy(embeddings_jobs).float()\n",
        "print(\"‚úîÔ∏è Conversion en tenseur r√©ussie :\", embeddings_jobs_tensor.shape)"
      ],
      "metadata": {
        "id": "PtQa1HXmprtt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wP6G3KAE9JIF",
        "egDQLQAGvsew",
        "SSi--gGRwp0x",
        "yIrDNyAoSrqF",
        "cEEJ_f6uFxge"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}